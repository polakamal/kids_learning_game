{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/polakamal/kids_learning_game/blob/master/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoKDuYrFI0PC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "ec610aab-4e37-4211-ec22-3814e53bfd95"
      },
      "source": [
        "\n",
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "import imageio\n",
        "from pandas import DataFrame\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "import csv\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn. ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk \n",
        "import string\n",
        "import re\n",
        "%matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 100) \n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K30ksqoUBqkJ"
      },
      "source": [
        "#-----------------------------------------------------------------pre processing--------------------------------------------------------------\n",
        "def load_data():\n",
        "    data = pd.read_csv('/content/Tweets.csv')\n",
        "    return data\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19XIZhsCB6Qv"
      },
      "source": [
        "df  = pd.DataFrame(load_data()[['tweet_id', 'text']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0Mc5ipEB7gi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "4cd1e54b-cac3-4b44-f98d-e0c2897b5d17"
      },
      "source": [
        "def tokenization(text):\n",
        "    text = re.split('\\W+', text)\n",
        "    return text\n",
        "\n",
        "df['Tweet_tokenized'] = df['text'].apply(lambda x: tokenization(x.lower()))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>Tweet_tokenized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>[, virginamerica, what, dhepburn, said, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
              "      <td>[, virginamerica, plus, you, ve, added, commercials, to, the, experience, tacky, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
              "      <td>[, virginamerica, i, didn, t, today, must, mean, i, need, to, take, another, trip, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;...</td>\n",
              "      <td>[, virginamerica, it, s, really, aggressive, to, blast, obnoxious, entertainment, in, your, gues...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
              "      <td>[, virginamerica, and, it, s, a, really, big, bad, thing, about, it]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...                                                                                      Tweet_tokenized\n",
              "0  570306133677760513  ...                                                            [, virginamerica, what, dhepburn, said, ]\n",
              "1  570301130888122368  ...                   [, virginamerica, plus, you, ve, added, commercials, to, the, experience, tacky, ]\n",
              "2  570301083672813571  ...                 [, virginamerica, i, didn, t, today, must, mean, i, need, to, take, another, trip, ]\n",
              "3  570301031407624196  ...  [, virginamerica, it, s, really, aggressive, to, blast, obnoxious, entertainment, in, your, gues...\n",
              "4  570300817074462722  ...                                 [, virginamerica, and, it, s, a, really, big, bad, thing, about, it]\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJd7u8gwDleq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "afd2a90c-665f-4443-99f9-fc13facc0f6a"
      },
      "source": [
        " import nltk\n",
        " nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkLbNGjIB9MV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "514c4de8-09fa-48ac-ecd7-156f772f05b1"
      },
      "source": [
        "stopword = nltk.corpus.stopwords.words('english')  \n",
        "def remove_stopwords(text):\n",
        "    text = [word for word in text if word not in stopword]\n",
        "    return text\n",
        "    \n",
        "df['Tweet_nonstop'] = df['Tweet_tokenized'].apply(lambda x: remove_stopwords(x))\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>Tweet_tokenized</th>\n",
              "      <th>Tweet_nonstop</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>[, virginamerica, what, dhepburn, said, ]</td>\n",
              "      <td>[, virginamerica, dhepburn, said, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
              "      <td>[, virginamerica, plus, you, ve, added, commercials, to, the, experience, tacky, ]</td>\n",
              "      <td>[, virginamerica, plus, added, commercials, experience, tacky, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
              "      <td>[, virginamerica, i, didn, t, today, must, mean, i, need, to, take, another, trip, ]</td>\n",
              "      <td>[, virginamerica, today, must, mean, need, take, another, trip, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;...</td>\n",
              "      <td>[, virginamerica, it, s, really, aggressive, to, blast, obnoxious, entertainment, in, your, gues...</td>\n",
              "      <td>[, virginamerica, really, aggressive, blast, obnoxious, entertainment, guests, faces, amp, littl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
              "      <td>[, virginamerica, and, it, s, a, really, big, bad, thing, about, it]</td>\n",
              "      <td>[, virginamerica, really, big, bad, thing]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>570300767074181121</td>\n",
              "      <td>@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's r...</td>\n",
              "      <td>[, virginamerica, seriously, would, pay, 30, a, flight, for, seats, that, didn, t, have, this, p...</td>\n",
              "      <td>[, virginamerica, seriously, would, pay, 30, flight, seats, playing, really, bad, thing, flying,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>570300616901320704</td>\n",
              "      <td>@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)</td>\n",
              "      <td>[, virginamerica, yes, nearly, every, time, i, fly, vx, this, ear, worm, won, t, go, away, ]</td>\n",
              "      <td>[, virginamerica, yes, nearly, every, time, fly, vx, ear, worm, go, away, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>570300248553349120</td>\n",
              "      <td>@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.c...</td>\n",
              "      <td>[, virginamerica, really, missed, a, prime, opportunity, for, men, without, hats, parody, there,...</td>\n",
              "      <td>[, virginamerica, really, missed, prime, opportunity, men, without, hats, parody, https, co, mwp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>570299953286942721</td>\n",
              "      <td>@virginamerica Well, I didn't…but NOW I DO! :-D</td>\n",
              "      <td>[, virginamerica, well, i, didn, t, but, now, i, do, d]</td>\n",
              "      <td>[, virginamerica, well]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>570295459631263746</td>\n",
              "      <td>@VirginAmerica it was amazing, and arrived an hour early. You're too good to me.</td>\n",
              "      <td>[, virginamerica, it, was, amazing, and, arrived, an, hour, early, you, re, too, good, to, me, ]</td>\n",
              "      <td>[, virginamerica, amazing, arrived, hour, early, good, ]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...                                                                                        Tweet_nonstop\n",
              "0  570306133677760513  ...                                                                  [, virginamerica, dhepburn, said, ]\n",
              "1  570301130888122368  ...                                     [, virginamerica, plus, added, commercials, experience, tacky, ]\n",
              "2  570301083672813571  ...                                    [, virginamerica, today, must, mean, need, take, another, trip, ]\n",
              "3  570301031407624196  ...  [, virginamerica, really, aggressive, blast, obnoxious, entertainment, guests, faces, amp, littl...\n",
              "4  570300817074462722  ...                                                           [, virginamerica, really, big, bad, thing]\n",
              "5  570300767074181121  ...  [, virginamerica, seriously, would, pay, 30, flight, seats, playing, really, bad, thing, flying,...\n",
              "6  570300616901320704  ...                          [, virginamerica, yes, nearly, every, time, fly, vx, ear, worm, go, away, ]\n",
              "7  570300248553349120  ...  [, virginamerica, really, missed, prime, opportunity, men, without, hats, parody, https, co, mwp...\n",
              "8  570299953286942721  ...                                                                              [, virginamerica, well]\n",
              "9  570295459631263746  ...                                             [, virginamerica, amazing, arrived, hour, early, good, ]\n",
              "\n",
              "[10 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiSfAc_uB-RO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "outputId": "b379846d-eac7-4f21-fd76-fc1fc3fc65a2"
      },
      "source": [
        "def remove_redundunt(text):\n",
        "    text = [word for word in text if word != 'virginamerica']\n",
        "    return text\n",
        "    \n",
        "df['Tweet_pure'] = df['Tweet_nonstop'].apply(lambda x: remove_redundunt(x))\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>Tweet_tokenized</th>\n",
              "      <th>Tweet_nonstop</th>\n",
              "      <th>Tweet_pure</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>[, virginamerica, what, dhepburn, said, ]</td>\n",
              "      <td>[, virginamerica, dhepburn, said, ]</td>\n",
              "      <td>[, dhepburn, said, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
              "      <td>[, virginamerica, plus, you, ve, added, commercials, to, the, experience, tacky, ]</td>\n",
              "      <td>[, virginamerica, plus, added, commercials, experience, tacky, ]</td>\n",
              "      <td>[, plus, added, commercials, experience, tacky, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
              "      <td>[, virginamerica, i, didn, t, today, must, mean, i, need, to, take, another, trip, ]</td>\n",
              "      <td>[, virginamerica, today, must, mean, need, take, another, trip, ]</td>\n",
              "      <td>[, today, must, mean, need, take, another, trip, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;...</td>\n",
              "      <td>[, virginamerica, it, s, really, aggressive, to, blast, obnoxious, entertainment, in, your, gues...</td>\n",
              "      <td>[, virginamerica, really, aggressive, blast, obnoxious, entertainment, guests, faces, amp, littl...</td>\n",
              "      <td>[, really, aggressive, blast, obnoxious, entertainment, guests, faces, amp, little, recourse]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
              "      <td>[, virginamerica, and, it, s, a, really, big, bad, thing, about, it]</td>\n",
              "      <td>[, virginamerica, really, big, bad, thing]</td>\n",
              "      <td>[, really, big, bad, thing]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>570300767074181121</td>\n",
              "      <td>@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's r...</td>\n",
              "      <td>[, virginamerica, seriously, would, pay, 30, a, flight, for, seats, that, didn, t, have, this, p...</td>\n",
              "      <td>[, virginamerica, seriously, would, pay, 30, flight, seats, playing, really, bad, thing, flying,...</td>\n",
              "      <td>[, seriously, would, pay, 30, flight, seats, playing, really, bad, thing, flying, va]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>570300616901320704</td>\n",
              "      <td>@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)</td>\n",
              "      <td>[, virginamerica, yes, nearly, every, time, i, fly, vx, this, ear, worm, won, t, go, away, ]</td>\n",
              "      <td>[, virginamerica, yes, nearly, every, time, fly, vx, ear, worm, go, away, ]</td>\n",
              "      <td>[, yes, nearly, every, time, fly, vx, ear, worm, go, away, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>570300248553349120</td>\n",
              "      <td>@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.c...</td>\n",
              "      <td>[, virginamerica, really, missed, a, prime, opportunity, for, men, without, hats, parody, there,...</td>\n",
              "      <td>[, virginamerica, really, missed, prime, opportunity, men, without, hats, parody, https, co, mwp...</td>\n",
              "      <td>[, really, missed, prime, opportunity, men, without, hats, parody, https, co, mwpg7grezp]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>570299953286942721</td>\n",
              "      <td>@virginamerica Well, I didn't…but NOW I DO! :-D</td>\n",
              "      <td>[, virginamerica, well, i, didn, t, but, now, i, do, d]</td>\n",
              "      <td>[, virginamerica, well]</td>\n",
              "      <td>[, well]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>570295459631263746</td>\n",
              "      <td>@VirginAmerica it was amazing, and arrived an hour early. You're too good to me.</td>\n",
              "      <td>[, virginamerica, it, was, amazing, and, arrived, an, hour, early, you, re, too, good, to, me, ]</td>\n",
              "      <td>[, virginamerica, amazing, arrived, hour, early, good, ]</td>\n",
              "      <td>[, amazing, arrived, hour, early, good, ]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...                                                                                     Tweet_pure\n",
              "0  570306133677760513  ...                                                                           [, dhepburn, said, ]\n",
              "1  570301130888122368  ...                                              [, plus, added, commercials, experience, tacky, ]\n",
              "2  570301083672813571  ...                                             [, today, must, mean, need, take, another, trip, ]\n",
              "3  570301031407624196  ...  [, really, aggressive, blast, obnoxious, entertainment, guests, faces, amp, little, recourse]\n",
              "4  570300817074462722  ...                                                                    [, really, big, bad, thing]\n",
              "5  570300767074181121  ...          [, seriously, would, pay, 30, flight, seats, playing, really, bad, thing, flying, va]\n",
              "6  570300616901320704  ...                                   [, yes, nearly, every, time, fly, vx, ear, worm, go, away, ]\n",
              "7  570300248553349120  ...      [, really, missed, prime, opportunity, men, without, hats, parody, https, co, mwpg7grezp]\n",
              "8  570299953286942721  ...                                                                                       [, well]\n",
              "9  570295459631263746  ...                                                      [, amazing, arrived, hour, early, good, ]\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP4mTAwQCAJQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "62c91d42-d215-4c4a-e011-79c54e8da16a"
      },
      "source": [
        "ps = nltk.PorterStemmer()\n",
        "\n",
        "def stemming(text):\n",
        "    text = [ps.stem(word) for word in text]\n",
        "    return text\n",
        "\n",
        "df['Tweet_stemmed'] = df['Tweet_pure'].apply(lambda x: stemming(x))\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>Tweet_tokenized</th>\n",
              "      <th>Tweet_nonstop</th>\n",
              "      <th>Tweet_pure</th>\n",
              "      <th>Tweet_stemmed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>[, virginamerica, what, dhepburn, said, ]</td>\n",
              "      <td>[, virginamerica, dhepburn, said, ]</td>\n",
              "      <td>[, dhepburn, said, ]</td>\n",
              "      <td>[, dhepburn, said, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
              "      <td>[, virginamerica, plus, you, ve, added, commercials, to, the, experience, tacky, ]</td>\n",
              "      <td>[, virginamerica, plus, added, commercials, experience, tacky, ]</td>\n",
              "      <td>[, plus, added, commercials, experience, tacky, ]</td>\n",
              "      <td>[, plu, ad, commerci, experi, tacki, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
              "      <td>[, virginamerica, i, didn, t, today, must, mean, i, need, to, take, another, trip, ]</td>\n",
              "      <td>[, virginamerica, today, must, mean, need, take, another, trip, ]</td>\n",
              "      <td>[, today, must, mean, need, take, another, trip, ]</td>\n",
              "      <td>[, today, must, mean, need, take, anoth, trip, ]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;...</td>\n",
              "      <td>[, virginamerica, it, s, really, aggressive, to, blast, obnoxious, entertainment, in, your, gues...</td>\n",
              "      <td>[, virginamerica, really, aggressive, blast, obnoxious, entertainment, guests, faces, amp, littl...</td>\n",
              "      <td>[, really, aggressive, blast, obnoxious, entertainment, guests, faces, amp, little, recourse]</td>\n",
              "      <td>[, realli, aggress, blast, obnoxi, entertain, guest, face, amp, littl, recours]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
              "      <td>[, virginamerica, and, it, s, a, really, big, bad, thing, about, it]</td>\n",
              "      <td>[, virginamerica, really, big, bad, thing]</td>\n",
              "      <td>[, really, big, bad, thing]</td>\n",
              "      <td>[, realli, big, bad, thing]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...                                                                    Tweet_stemmed\n",
              "0  570306133677760513  ...                                                             [, dhepburn, said, ]\n",
              "1  570301130888122368  ...                                           [, plu, ad, commerci, experi, tacki, ]\n",
              "2  570301083672813571  ...                                 [, today, must, mean, need, take, anoth, trip, ]\n",
              "3  570301031407624196  ...  [, realli, aggress, blast, obnoxi, entertain, guest, face, amp, littl, recours]\n",
              "4  570300817074462722  ...                                                      [, realli, big, bad, thing]\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuLqvO2CCBLY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "outputId": "5e121ac2-2e72-48bf-d986-10e4792c0dd6"
      },
      "source": [
        "def remove_punct(text):\n",
        "    text  = \" \".join([char for char in text if char not in string.punctuation])\n",
        "    text = re.sub('[0-9]+', '', text)\n",
        "    return text\n",
        "\n",
        "df['final_data'] = df['Tweet_stemmed'].apply(lambda x: remove_punct(x))\n",
        "df.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>text</th>\n",
              "      <th>Tweet_tokenized</th>\n",
              "      <th>Tweet_nonstop</th>\n",
              "      <th>Tweet_pure</th>\n",
              "      <th>Tweet_stemmed</th>\n",
              "      <th>final_data</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>[, virginamerica, what, dhepburn, said, ]</td>\n",
              "      <td>[, virginamerica, dhepburn, said, ]</td>\n",
              "      <td>[, dhepburn, said, ]</td>\n",
              "      <td>[, dhepburn, said, ]</td>\n",
              "      <td>dhepburn said</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>@VirginAmerica plus you've added commercials to the experience... tacky.</td>\n",
              "      <td>[, virginamerica, plus, you, ve, added, commercials, to, the, experience, tacky, ]</td>\n",
              "      <td>[, virginamerica, plus, added, commercials, experience, tacky, ]</td>\n",
              "      <td>[, plus, added, commercials, experience, tacky, ]</td>\n",
              "      <td>[, plu, ad, commerci, experi, tacki, ]</td>\n",
              "      <td>plu ad commerci experi tacki</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I need to take another trip!</td>\n",
              "      <td>[, virginamerica, i, didn, t, today, must, mean, i, need, to, take, another, trip, ]</td>\n",
              "      <td>[, virginamerica, today, must, mean, need, take, another, trip, ]</td>\n",
              "      <td>[, today, must, mean, need, take, another, trip, ]</td>\n",
              "      <td>[, today, must, mean, need, take, anoth, trip, ]</td>\n",
              "      <td>today must mean need take anoth trip</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp;...</td>\n",
              "      <td>[, virginamerica, it, s, really, aggressive, to, blast, obnoxious, entertainment, in, your, gues...</td>\n",
              "      <td>[, virginamerica, really, aggressive, blast, obnoxious, entertainment, guests, faces, amp, littl...</td>\n",
              "      <td>[, really, aggressive, blast, obnoxious, entertainment, guests, faces, amp, little, recourse]</td>\n",
              "      <td>[, realli, aggress, blast, obnoxi, entertain, guest, face, amp, littl, recours]</td>\n",
              "      <td>realli aggress blast obnoxi entertain guest face amp littl recours</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing about it</td>\n",
              "      <td>[, virginamerica, and, it, s, a, really, big, bad, thing, about, it]</td>\n",
              "      <td>[, virginamerica, really, big, bad, thing]</td>\n",
              "      <td>[, really, big, bad, thing]</td>\n",
              "      <td>[, realli, big, bad, thing]</td>\n",
              "      <td>realli big bad thing</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>570300767074181121</td>\n",
              "      <td>@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\nit's r...</td>\n",
              "      <td>[, virginamerica, seriously, would, pay, 30, a, flight, for, seats, that, didn, t, have, this, p...</td>\n",
              "      <td>[, virginamerica, seriously, would, pay, 30, flight, seats, playing, really, bad, thing, flying,...</td>\n",
              "      <td>[, seriously, would, pay, 30, flight, seats, playing, really, bad, thing, flying, va]</td>\n",
              "      <td>[, serious, would, pay, 30, flight, seat, play, realli, bad, thing, fli, va]</td>\n",
              "      <td>serious would pay  flight seat play realli bad thing fli va</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>570300616901320704</td>\n",
              "      <td>@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)</td>\n",
              "      <td>[, virginamerica, yes, nearly, every, time, i, fly, vx, this, ear, worm, won, t, go, away, ]</td>\n",
              "      <td>[, virginamerica, yes, nearly, every, time, fly, vx, ear, worm, go, away, ]</td>\n",
              "      <td>[, yes, nearly, every, time, fly, vx, ear, worm, go, away, ]</td>\n",
              "      <td>[, ye, nearli, everi, time, fli, vx, ear, worm, go, away, ]</td>\n",
              "      <td>ye nearli everi time fli vx ear worm go away</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>570300248553349120</td>\n",
              "      <td>@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.c...</td>\n",
              "      <td>[, virginamerica, really, missed, a, prime, opportunity, for, men, without, hats, parody, there,...</td>\n",
              "      <td>[, virginamerica, really, missed, prime, opportunity, men, without, hats, parody, https, co, mwp...</td>\n",
              "      <td>[, really, missed, prime, opportunity, men, without, hats, parody, https, co, mwpg7grezp]</td>\n",
              "      <td>[, realli, miss, prime, opportun, men, without, hat, parodi, http, co, mwpg7grezp]</td>\n",
              "      <td>realli miss prime opportun men without hat parodi http co mwpggrezp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>570299953286942721</td>\n",
              "      <td>@virginamerica Well, I didn't…but NOW I DO! :-D</td>\n",
              "      <td>[, virginamerica, well, i, didn, t, but, now, i, do, d]</td>\n",
              "      <td>[, virginamerica, well]</td>\n",
              "      <td>[, well]</td>\n",
              "      <td>[, well]</td>\n",
              "      <td>well</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>570295459631263746</td>\n",
              "      <td>@VirginAmerica it was amazing, and arrived an hour early. You're too good to me.</td>\n",
              "      <td>[, virginamerica, it, was, amazing, and, arrived, an, hour, early, you, re, too, good, to, me, ]</td>\n",
              "      <td>[, virginamerica, amazing, arrived, hour, early, good, ]</td>\n",
              "      <td>[, amazing, arrived, hour, early, good, ]</td>\n",
              "      <td>[, amaz, arriv, hour, earli, good, ]</td>\n",
              "      <td>amaz arriv hour earli good</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             tweet_id  ...                                                           final_data\n",
              "0  570306133677760513  ...                                                        dhepburn said\n",
              "1  570301130888122368  ...                                         plu ad commerci experi tacki\n",
              "2  570301083672813571  ...                                 today must mean need take anoth trip\n",
              "3  570301031407624196  ...   realli aggress blast obnoxi entertain guest face amp littl recours\n",
              "4  570300817074462722  ...                                                 realli big bad thing\n",
              "5  570300767074181121  ...          serious would pay  flight seat play realli bad thing fli va\n",
              "6  570300616901320704  ...                         ye nearli everi time fli vx ear worm go away\n",
              "7  570300248553349120  ...  realli miss prime opportun men without hat parodi http co mwpggrezp\n",
              "8  570299953286942721  ...                                                                 well\n",
              "9  570295459631263746  ...                                           amaz arriv hour earli good\n",
              "\n",
              "[10 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqtuoTJUCDX4"
      },
      "source": [
        "twittes_pre=[]\n",
        "for i in df['final_data']:\n",
        "  twittes_pre.append(i)\n",
        "  #-----------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECSCvjQYr9jW"
      },
      "source": [
        "\n",
        "#----------------------------------------------------------------RepareData(preprocessed)------------------------------------------------------------------------\n",
        "\n",
        "Data=[]\n",
        "op=[]\n",
        "\n",
        "with open('/content/Tweets.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    for a,row in enumerate(reader):\n",
        "      Data.append(row[10])\n",
        "      op.append(row[1])\n",
        "    Data=Data[1:]\n",
        "    op=op[1:]    \n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(twittes_pre)\n",
        "Labels=[]\n",
        "Twitts=X.toarray()\n",
        "for i in op:\n",
        "  if(i=='neutral'):\n",
        "    Labels.append(1)\n",
        "  elif(i=='positive'):\n",
        "    Labels.append(0)\n",
        "  else:\n",
        "    Labels.append(2)\n",
        "True_Labels=Labels\n",
        "x_train, x_test, y_train, y_test = train_test_split( Twitts,Labels, test_size=0.30, random_state=42, shuffle = True)\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8r3hPguvRiu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "408457e7-e32b-4f66-ace5-e218cc6e08b8"
      },
      "source": [
        "#--------------------------------------------------------list_to_array----------------------------------------------------------------------------------------\n",
        "\n",
        "x_train=np.array(x_train)\n",
        "x_test=np.array(x_test)\n",
        "y_train=np.array(y_train)\n",
        "y_test=np.array(y_test)\n",
        "print(f'train images: {x_train.shape}')\n",
        "print(f'train labels: {y_train.shape}')\n",
        "print(f'test images: {x_test.shape}')\n",
        "print(f'test labels: {y_test.shape}')\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train images: (10248, 10388)\n",
            "train labels: (10248,)\n",
            "test images: (4392, 10388)\n",
            "test labels: (4392,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_Eja0yiDyti"
      },
      "source": [
        "\n",
        "#len(la)\n",
        "postive=0\n",
        "negative=0\n",
        "neutral=0\n",
        "for i in True_Labels:\n",
        "  if(i==0):\n",
        "   postive=postive+1\n",
        "  elif(i==1): \n",
        "    negative=negative+1\n",
        "  else:\n",
        "    neutral=neutral+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPKwf2wgKoaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "72109ded-7334-4e67-9de9-f6514eef7430"
      },
      "source": [
        "print(f'postive :{(postive/14640)* 100:.2f}%')\n",
        "print(f'negative :{(negative/14640)* 100:.2f}%')\n",
        "print(f'neutral:{(neutral/14640)* 100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "postive :16.14%\n",
            "negative :21.17%\n",
            "neutral:62.69%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9uZ6aJkxQzv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "252b65e1-f9db-473b-dadd-30b38940da0d"
      },
      "source": [
        "\n",
        "\n",
        "#--------------------------------------------5 models(lr,svm,knn,naive bais Multinomial,decession tree)--------------------------------------------\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "lr = LogisticRegression()\n",
        "svm = svm.SVC()\n",
        "Knn= KNeighborsClassifier()\n",
        "MultinomialNB=MultinomialNB()\n",
        "\n",
        "\n",
        "#to learn model->\n",
        "dt.fit(x_train,y_train) \n",
        "lr.fit(x_train,y_train)\n",
        "svm.fit(x_train,y_train)\n",
        "Knn.fit(x_train,y_train)\n",
        "MultinomialNB.fit(x_train,y_train)\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiOPiBNdK4IT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "c41a5f82-c397-4912-8f9f-87aabc175dec"
      },
      "source": [
        "\n",
        "#--------------------------------------------------[print accuarcy of our models-----------------------------------------------------------------\n",
        "\n",
        "\n",
        "print(f'train Accuracy of Decession tree: {dt.score(x_train,y_train) * 100:.2f}%' )\n",
        "print(f'test Accuracy of Decession tree:{dt.score(x_test,y_test)* 100:.2f}%')\n",
        "print(f'train Sccuracy of Logistic:{lr.score(x_train,y_train)* 100:.2f}%')\n",
        "print(f'test Accuracy of Logistic:{lr.score(x_test,y_test)* 100:.2f}%')\n",
        "print(f'train Accuracy of naive bais Multinomial:{MultinomialNB.score(x_train,y_train)* 100:.2f}%')\n",
        "print(f'test Accuracy of naive bais Multinomial:{MultinomialNB.score(x_test,y_test)* 100:.2f}%')\n",
        "print(f'train Sccuracy of Knn:{Knn.score(x_train,y_train)* 100:.2f}%')\n",
        "print(f'test Sccuracy of Knn:{ Knn.score(x_test,y_test)* 100:.2f}%')\n",
        "print(f'train Accuracy of svm:{svm.score(x_train,y_train)* 100:.2f}%')\n",
        "print(f'test Accuracy of svm:{svm.score(x_test,y_test)* 100:.2f}%')\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train Accuracy of Decession tree: 99.61%\n",
            "test Accuracy of Decession tree:69.08%\n",
            "train Sccuracy of Logistic:86.93%\n",
            "test Accuracy of Logistic:78.94%\n",
            "train Accuracy of naive bais Multinomial:71.69%\n",
            "test Accuracy of naive bais Multinomial:69.72%\n",
            "train Sccuracy of Knn:78.49%\n",
            "test Sccuracy of Knn:68.35%\n",
            "train Accuracy of svm:95.37%\n",
            "test Accuracy of svm:78.92%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxPJbJwPLT0N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "792f3b6c-d9cc-44f8-80ac-742e26fe4b2c"
      },
      "source": [
        "#-------------------------------------------------------ensemple between logistic regression and decession tree-----------------------------------------------------\n",
        "\n",
        "evc = VotingClassifier( estimators= [('lr',lr),('dt',dt)])\n",
        "evc.fit(x_train,y_train)\n",
        "print(f'Test acc:{evc.score(x_test,y_test)* 100:.2f}%')  \n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test acc:72.15%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqV3zn4m2gPE"
      },
      "source": [
        "\n",
        "#---------------------------------------------------------------------------predict in 5 models--------------------------------------------------------------------------------\n",
        "y_pred_lr = lr.predict(x_test)\n",
        "y_pred_Dt = dt.predict(x_test)\n",
        "y_pred_Knn = Knn.predict(x_test)\n",
        "y_pred_Naive = MultinomialNB.predict(x_test)\n",
        "y_pred_svm = svm.predict(x_test)\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWfjUoQzBjqA"
      },
      "source": [
        "\n",
        "#----------------------------------------------------------------------------Generate evaluation Matrices-----------------------------------------------------------------------------\n",
        "CR_lr=classification_report(y_test,y_pred_lr)\n",
        "CR_dt=classification_report(y_test,y_pred_Dt)\n",
        "CR_Knn=classification_report(y_test,y_pred_Knn)\n",
        "CR_naive=classification_report(y_test,y_pred_Naive)\n",
        "CR_svm=classification_report(y_test,y_pred_svm)\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ahHVFXdCG9r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f298d98f-02ef-4e42-f2c0-9c7849546820"
      },
      "source": [
        "#------------------------------------------------------  ---------------Print evaluation matrix for each model-----------------------------------------------------------\n",
        "\n",
        "print('Logitic regression','\\n',CR_lr )\n",
        "print('-----------------------------------------------------------------------------------------------')\n",
        "print('Decession Tree','\\n',CR_dt)\n",
        "print('-----------------------------------------------------------------------------------------------')\n",
        "print('Svm','\\n',CR_svm )\n",
        "print('-----------------------------------------------------------------------------------------------')\n",
        "print('MultiNomial NaiveBayes','\\n',CR_naive)\n",
        "print('-----------------------------------------------------------------------------------------------')\n",
        "print('Knn','\\n',CR_Knn )\n",
        "\n",
        "#------------------------------------------------------------------------------------------------------------------------------------------------------------------"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logitic regression \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.60      0.69       694\n",
            "           1       0.66      0.47      0.55       884\n",
            "           2       0.81      0.94      0.87      2814\n",
            "\n",
            "    accuracy                           0.79      4392\n",
            "   macro avg       0.76      0.67      0.70      4392\n",
            "weighted avg       0.78      0.79      0.78      4392\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n",
            "Decession Tree \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.53      0.57       694\n",
            "           1       0.43      0.39      0.41       884\n",
            "           2       0.78      0.83      0.80      2814\n",
            "\n",
            "    accuracy                           0.69      4392\n",
            "   macro avg       0.61      0.58      0.59      4392\n",
            "weighted avg       0.68      0.69      0.68      4392\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n",
            "Svm \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.60      0.70       694\n",
            "           1       0.72      0.41      0.52       884\n",
            "           2       0.79      0.96      0.87      2814\n",
            "\n",
            "    accuracy                           0.79      4392\n",
            "   macro avg       0.78      0.65      0.69      4392\n",
            "weighted avg       0.78      0.79      0.77      4392\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n",
            "MultiNomial NaiveBayes \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.17      0.29       694\n",
            "           1       0.75      0.17      0.27       884\n",
            "           2       0.69      0.99      0.81      2814\n",
            "\n",
            "    accuracy                           0.70      4392\n",
            "   macro avg       0.79      0.44      0.46      4392\n",
            "weighted avg       0.74      0.70      0.62      4392\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n",
            "Knn \n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.65      0.59       694\n",
            "           1       0.43      0.52      0.47       884\n",
            "           2       0.84      0.74      0.79      2814\n",
            "\n",
            "    accuracy                           0.68      4392\n",
            "   macro avg       0.60      0.64      0.62      4392\n",
            "weighted avg       0.71      0.68      0.69      4392\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}